# Portal de conversaciones

üëã ¬°Bienvenidos a nuestro proyecto! 

## Descripci√≥n del proyecto

### ü§î Problema
La Cruz Roja se enfrenta al problema de que los procesos de acogida y valoraci√≥n, realizados como primer contacto con un usuario a trav√©s de entrevistas y formularios espec√≠ficos, pueden ser largos y fr√≠os, lo que puede afectar negativamente la experiencia del usuario y reducir su participaci√≥n en los programas y servicios de la ONG. Para mejorar la eficiencia de este proceso y crear un primer contacto m√°s c√°lido y acogedor, Cruz Roja busca una soluci√≥n innovadora que automatice el proceso de acogida y valoraci√≥n y permita a los entrevistadores obtener los beneficios de una mejor atenci√≥n sin tener que completar formularios. 


### üí° Soluci√≥n

Para abordar el problema de eficiencia en el proceso de valoraci√≥n, hemos creado una soluci√≥n innovadora llamada "Portal de conversaciones". Esta plataforma es una herramienta de software basada en la nube, alojada en Amazon Web Services (AWS), que permite a los entrevistadores de Cruz Roja realizar la valoraci√≥n de los usuarios sin tener que llenar formularios, lo que reduce significativamente el tiempo necesario para completar el proceso.

La plataforma utiliza tecnolog√≠as como Transcribe, OpenAI i Deepgram para analizar y procesar los datos de la entrevista y generar un informe detallado del usuario. Adem√°s, la plataforma ofrece una interfaz visual intuitiva para que los entrevistadores puedan interactuar con los resultados de la valoraci√≥n y hacer un seguimiento de los usuarios a lo largo del tiempo.

El "Portal de conversaciones" es una soluci√≥n innovadora ya que puede procesar en tiempo real una conversaci√≥n y marcar los puntos clave o preguntas que ya se han hablado en ese momento. Adem√°s, gracias a su dise√±o modular, la plataforma puede adaptarse f√°cilmente a las necesidades espec√≠ficas de la Cruz Roja y puede extenderse a otras aplicaciones.

## Table of Contents

- [Portal de conversaciones](#portal-de-conversaciones)
  - [Descripci√≥n del proyecto](#descripci√≥n-del-proyecto)
    - [ü§î Problema](#-problema)
    - [üí° Soluci√≥n](#-soluci√≥n)
  - [Table of Contents](#table-of-contents)
  - [Diagrama de Arquitectura](#diagrama-de-arquitectura)
  - [Descripci√≥n t√©cnica](#descripci√≥n-t√©cnica)
    - [Software](#software)
      - [Repositorios](#repositorios)
      - [Diagrama relacional](#diagrama-relacional)
      - [üîß Funcionalidades](#-funcionalidades)
        - [1. Iniciar sesi√≥n](#1-iniciar-sesi√≥n)
        - [2. Modificar mi perfil](#2-modificar-mi-perfil)
        - [3. A√±adir un usuario](#3-a√±adir-un-usuario)
        - [4. Ordenar usuarios](#4-ordenar-usuarios)
        - [5. Subir conversaci√≥n](#5-subir-conversaci√≥n)
        - [6. Ver transcripci√≥n](#6-ver-transcripci√≥n)
        - [7. Saber m√°s](#7-saber-m√°s)
        - [8. Reproducir conversaci√≥n](#8-reproducir-conversaci√≥n)
        - [9. Ficha de usuario](#9-ficha-de-usuario)
        - [10. Grabar conversaci√≥n](#10-grabar-conversaci√≥n)
        - [11. Buscador](#11-buscador)
        - [12. Ver notificaciones](#12-ver-notificaciones)
    - [Arquitectura Cloud](#arquitectura-cloud)
      - [Descripci√≥n](#descripci√≥n)
      - [Repositorios](#repositorios-1)
      - [AWS Services](#aws-services)
        - [EC2, ECS, ECR, Fargate, ALB](#ec2-ecs-ecr-fargate-alb)
        - [Transcribe](#transcribe)
        - [Aurora RDS](#aurora-rds)
        - [Opensearch](#opensearch)
        - [IAM](#iam)
        - [S3](#s3)
        - [Cloudfront](#cloudfront)
        - [Cloudwatch, SNS](#cloudwatch-sns)
        - [ACM](#acm)
        - [Route53](#route53)
        - [Backups](#backups)
        - [Config](#config)
        - [Cloudtrail](#cloudtrail)
      - [Recuperaci√≥n de desastres](#recuperaci√≥n-de-desastres)
        - [Politica de backups](#politica-de-backups)
        - [DRP Test](#drp-test)
        - [MTTR ( Mean Time To Recover )](#mttr--mean-time-to-recover-)
      - [AWS Pilars](#aws-pilars)
        - [Excelencia operativa](#excelencia-operativa)
        - [Seguridad](#seguridad)
        - [Fiabilidad](#fiabilidad)
        - [Eficiencia del rendimiento](#eficiencia-del-rendimiento)
        - [Optimizaci√≥n de costos](#optimizaci√≥n-de-costos)
        - [Sostenibilidad](#sostenibilidad)
      - [CI/CD](#cicd)
      - [Costes](#costes)
  - [Local development](#Ô∏è-local-development)
  - [GDPR](#gdpr)
  - [üé• Demo](#Ô∏è-demo)
  - [üë• Team Members](#-team-members)
  - [‚è≠Ô∏è Trabajo futuro](#Ô∏è-trabajo-futuro)
  - [üéâ Conclusi√≥n](#-conclusi√≥n)

## Diagrama de Arquitectura

![Arquitectura AWS](img/architecture.svg)

## Descripci√≥n t√©cnica

### Software

#### Repositorios

- [Frontend](https://github.com/JPG-squad/front) 

[![React Icon](https://img.shields.io/badge/-React-blue?logo=react&logoColor=white&style=flat)](https://reactjs.org/)
[![JavaScript Icon](https://img.shields.io/badge/-JavaScript-yellow?logo=javascript&logoColor=white&style=flat)](https://www.javascript.com/)


- [Backend](https://github.com/JPG-squad/back)

[![Python Icon](https://img.shields.io/badge/-Python-yellow?logo=python&logoColor=white&style=flat)](https://www.python.org/)
[![Django Icon](https://img.shields.io/badge/-Django-green?logo=django&logoColor=white&style=flat)](https://www.djangoproject.com/)
[![PostgreSQL Icon](https://img.shields.io/badge/-PostgreSQL-blue?logo=postgresql&logoColor=white&style=flat)](https://www.postgresql.org/)
[![OpenSearch Icon](https://img.shields.io/badge/-OpenSearch-blue?logo=amazon-aws&logoColor=white&style=flat)](https://opensearch.org/)
[![Docker Icon](https://img.shields.io/badge/-Docker-blue?logo=docker&logoColor=white&style=flat)](https://www.docker.com/)


- [Research IA ( Not used )](https://github.com/JPG-squad/back)
  
[![Python Icon](https://img.shields.io/badge/-Python-yellow?logo=python&logoColor=white&style=flat)](https://www.python.org/)
[![FastAPI Icon](https://img.shields.io/badge/-FastAPI-blue?logo=fastapi&logoColor=white&style=flat)](https://fastapi.tiangolo.com/)
[![Docker Icon](https://img.shields.io/badge/-Docker-blue?logo=docker&logoColor=white&style=flat)](https://www.docker.com/)


#### Diagrama relacional

A continuaci√≥n, se muestra el diagrama relacional de los modelos de la base de datos utilizados en la plataforma:

![Ejemplo de imagen](img/HackatonModel.drawio.png)

#### üîß Funcionalidades

A continuaci√≥n, se enumeran todas las funcionalidades que ofrece el Portal de conversaciones:

- [1. Iniciar sesi√≥n](#1-iniciar-sesi√≥n)
- [2. Modificar mi perfil](#2-modificar-mi-perfil)
- [3. A√±adir un usuario](#3-a√±adir-un-usuario)
- [4. Ordenar usuarios](#4-ordenar-usuarios)
- [5. Subir conversaci√≥n](#5-subir-conversaci√≥n)
- [6. Ver transcripci√≥n](#6-ver-transcripci√≥n)
- [7. Saber m√°s](#7-saber-m√°s)
- [8. Reproducir conversaci√≥n](#8-reproducir-conversaci√≥n)
- [9. Ficha de usuario](#9-ficha-de-usuario)
- [10. Grabar conversaci√≥n](#10-grabar-conversaci√≥n)
- [11. Buscador](#11-buscador)
- [12. Ver notificaciones](#12-ver-notificaciones)

##### 1. Iniciar sesi√≥n
La funcionalidad de inicio de sesi√≥n es la primera en la plataforma, permitiendo a los usuarios acceder a su cuenta a trav√©s de su correo electr√≥nico y contrase√±a registrados previamente. Estos datos deben estar almacenados en nuestra base de datos como un "Empleado".

**Implementaci√≥n t√©cnica**
> El sistema de autenticaci√≥n es un sistema senzillo, con un **token** que se almacena en la base de datos y se envia al cliente para que lo almacene en el **local storage**. Este token se envia en cada peticion al backend para que este pueda identificar al usuario. Este token se genera cuando el usuario introduce correctamente su usuario y contrasenya, y se eliminia al cerrar sesi√≥n.

##### 2. Modificar mi perfil
Esta funcionalidad te permite actualizar la informaci√≥n de tu cuenta en la plataforma. Para acceder a esta funcionalidad, haz clic en el icono de tu perfil ubicado en la esquina superior derecha de la pantalla. Ver√°s que aparecer√° un men√∫ desplegable con varias opciones, entre ellas "Editar mi perfil". Al seleccionar esta opci√≥n, podr√°s editar tus datos personales, por ahora solo tu nombre. Tambi√©n puedes cerrar sesi√≥n desde este mismo men√∫.

La funcionalidad ahora mismo no tiene mucho sentido pero esta pensada por si en un futuro un empleado tiene m√°s campos editables.

**Implementaci√≥n t√©cnica**
> Para modificar el perfil, el usuario dispone de una model d√≥nde permitimos cambiar los campos que s√≥n ediatables del model del trabajador de la Cruz Roja. Para ello, se hace una petici√≥n al backend con los datos del usuario y se actualizan en la base de datos.

##### 3. A√±adir un usuario
Para empezar una conversaci√≥n, es necesario a√±adir un usuario. En la parte izquierda de la plataforma, encontrar√°s el listado de usuarios y podr√°s agregar uno nuevo. Solo necesitas introducir su nombre y correo electr√≥nico, y opcionalmente, su n√∫mero de tel√©fono. Cuando agregues al usuario, aparecer√° en la parte superior de la lista y podr√°s comenzar una conversaci√≥n con √©l o ella desde la parte derecha de la plataforma.

**Implementaci√≥n T√©cnica**
> Un trabajador de la cruz roja a√±ade un nueve paciente des de la modal que se muestra en en frontend. Luego, estos datos sirven para crear una nueva instancia del model que representa a un usuario en la base de datos. Este usuario queda enlazado con el trabajador de la cruz roja.

##### 4. Ordenar usuarios
En la secci√≥n de listado de usuarios, encontrar√°s diversas opciones para ordenarlos y facilitar su b√∫squeda. Podr√°s ordenarlos alfab√©ticamente o por el grado de interacci√≥n con ellos. Esto te permitir√° encontrar r√°pidamente al usuario con el que quieres conversar y mejorar la eficiencia de tus interacciones en la plataforma.

**Implementaci√≥n T√©cnica**
> Des del frontend, somos capaces de ordenar el listado de usuarios por dos criterios: aflab√©ticamente y por "**frecuencia de interacci√≥n**" (d√≥nde el √∫ltimo usuario con el que se ha hecho una acci√≥n se muestra arriba o abajo, dependiendo de la ordenaci√≥n). Para la ordenaci√≥n alfab√©tica, lo hace puramente el frontend. Para la ordenaci√≥n por interacci√≥n, se utiliza el campo *updated_at* de la base de datos del model usuario. Este campo se actualiza cada vez que se hace una acci√≥n con este.

##### 5. Subir conversaci√≥n

Una vez seleccionado un usuario, ver√°s un bot√≥n rojo con un icono de "+" en la parte inferior derecha de la plataforma. Al pulsar este bot√≥n, se abrir√°n dos opciones. 

La primera opci√≥n te permitir√° subir una conversaci√≥n en m√∫ltiples formatos, como Wav, Mp3 y Webm. Adem√°s, podr√°s seleccionar el modelo de inteligencia artificial que procesar√° la conversaci√≥n. 

Por ahora, puedes escoger entre AWS Transcribe y Deepgram. Mientras se procesa la conversaci√≥n, ver√°s un mensaje de carga en la parte inferior central de la plataforma. Una gran ventaja de nuestra plataforma es que podr√°s seguir navegando e interactuando con ella mientras se procesa la conversaci√≥n. 

Una vez se haya procesado con √©xito, aparecer√° un mensaje y una notificaci√≥n. La conversaci√≥n se situar√° arriba del todo en la lista de conversaciones de ese usuario, ya que por defecto est√°n ordenadas por orden de creaci√≥n. Se habr√° generado autom√°ticamente un t√≠tulo identificativo y una descripci√≥n breve para esa conversaci√≥n. 

Adem√°s, habr√° tres botones que explicaremos m√°s adelante en otras funcionalidades. 

Si esta es la primera conversaci√≥n, tambi√©n se generar√° autom√°ticamente una ficha de usuario, de la cual hablaremos m√°s adelante.

**Implementaci√≥n T√©cnica**
> Para subir una conversaci√≥n, el usuario selecciona un archivo y lo sube utilizando el bot√≥n de subir conversaci√≥n. En este momento, el front comprueba que el fichero es un fichero de audio y de alguno de los formatos que soportamos. Adem√°s, el usuario puede elegir qu√© modelo de **IA** desea utilitzar para transcribir la conversaci√≥n. Tanto el fichero como este par√°metro son pasados al backend para hacer la acci√≥n de subir la conversaci√≥n (uno como datos del POST y el otro como query parameter). <br> <br>
> Una vez el backend recibe la petici√≥n, la manda para transcribir a la inteligiencia artificial seleccionada: **AWS Transcribe** o **Deepgram** (siendo **AWS Transcribe** la que se utiliza por defecto porque ha sido la que nos da mejor resultados de diarizaci√≥n). Cuando esta termina, el backend recibe la transcripci√≥n. Esta es una implementaci√≥n d√≥nde el backend espera mientras se va procesando, haciendo **POLLING** para ver cuando est√°. La implementaci√≥n concreta de cada modelo utilizado se puede ver en el c√≥digo, as√≠ como los parametros utilizados para cada uno (que han sido el resultado de hacer pruebas e ir refinandolo). En ambas, pedimos que la IA haga diagn√≥stico de los hablantes, para poder identificarlos en la transcripci√≥n (*diarization*). <br> <br>
> Una vez recibida la respuesta, se trata el resultado seg√∫n el formato de salida que nos devuelve, y creamos una **estructura en forma de chat**, construyendo frases intercaladas entre los hablantes que hayan en la conversaci√≥n. <br> <br>
> Despu√©s, con esta transcirpci√≥n que hemos generado, **intentamos responder las preguntas del formulario** que tenemos el objetivo de rellenar (la "*ficha de usuario*", como lo llamamos nosotros). Para hacerlo, utilizamos dos funciones que hemos creado, que utilizan **ChatGPT de OpenAI** para intentar determinar dos coses: si se han respondido o no las preguntas y, para las que s√≠ que detecta que se ha hablado o respondido, que genere una respuesta. Con esto, guardamos este "**checklist**" de lo respondido en modelos de la base de datos, donde guardamos las repspuestas para este usuario (que nos serviran para crear la ficha de usuario). <br> <br>
> Por √∫ltimo, tambi√©n utilizamos el **ChatGPT** para generar un **t√≠tulo** y **descripci√≥n** autom√°ticos para la conversaci√≥n, que es lo que terminamos devolviendo en la llamada del frontend.

##### 6. Ver transcripci√≥n
En la carta de una conversaci√≥n ver√°s una serie de botones en la parte inferior. El primer bot√≥n, te permitir√° ver la transcripci√≥n generada por el modelo de inteligencia artificial que has seleccionado al subir la conversaci√≥n. La transcripci√≥n se mostrar√° en un formato de chat, donde podr√°s ver el di√°logo que ha tenido lugar durante la conversaci√≥n. Este formato hace que la transcripci√≥n sea m√°s f√°cil de leer y comprender, permitiendo al usuario identificar r√°pidamente lo que se ha dicho en cada momento.

**Implementaci√≥n T√©cnica**
> Cu√°ndo el usario pulsa sobre el bot√≥n de transcripci√≥n de una conversaci√≥n, se hace una llamada al backend indicando de que usuario y qu√© conversaci√≥n concreta se quiere la transcripci√≥n. Esta transcripci√≥n puede ser el resultado de una conversaci√≥n grabada en tiempo real desde nuestra apliaci√≥n (que se explica en el punto 10) o bien una conversaci√≥n que se ha subido a la plataforma en forma de fichero de audio (que se explica en el punto 5). <br> <br>
> El backend simplemente devuelve lo que tiene almazenado de esta conversaci√≥n el campo de transcripci√≥n (que es un campo json), que es el campo que rellenamos despu√©s de haber procesado las conversaciones y formateado en forma de chat (un array de objetos json, d√≥nde cada objeto tiene dos campos: el texto y el hablante). <br> <br>


##### 7. Saber m√°s
Al lado del bot√≥n para ver la transcripci√≥n, ver√°s otro bot√≥n con el texto "Saber m√°s". Al hacer clic en √©l, se abrir√° una ventana emergente con un campo de texto en el que puedes escribir cualquier pregunta relacionada con la conversaci√≥n o cualquier detalle que se te haya pasado durante la entrevista. Una vez hayas escrito alguna pregunta, va a aparecer una respuesta en pocos segundos.

Esta funcionalidad te permite asegurarte de que no se te escapa ning√∫n detalle importante y te permite revivir cualquier pregunta que hayas hecho durante la entrevista para recordar las respuestas del usuario.

Adem√°s, esta funcionalidad es muy vers√°til y te permite hacer cualquier tipo de pregunta en el momento. Todo lo que tienes que hacer es escribir la pregunta en el campo de texto y presionar "Enter". As√≠ de f√°cil.

**Implementaci√≥n T√©cnica**
> Cu√°ndo el usuario utiliza la funcionalidad de saber m√°s, se hace una llamada pasando el "*prompt*" o pregunta que el usario quiere saber sobre una determinada conversaci√≥n. Cu√°ndo el backend lo recibe, recupera la trascripci√≥n de la conversaci√≥n sobre la que se est√° preguntado, que nos sirve como contexto para poder responder. <br> <br>
> Luego, con el contexto y la pregunta, se utiliza **ChatGPT** para responderla. Se le pasa toda la transcripci√≥n y, adem√°s, a√±adimos un prompt adicional nosotros para mejorar el tipo de respuestas que hace (que han sido resultado de ir haciendo pruebas y refinando). Una vez hemos respondido la pregunta, se devuelve el resultado al frontend, despu√©s de haber guardado este en la base de datos, de tal forma que la pr√≥xima vez que el usuario entre en la secci√≥n de ver m√°s de esta misma conversaci√≥n, va a ver lo que ya hab√≠a preguntado anteriormente para que no lo tenga que volver a preguntar.

##### 8. Reproducir conversaci√≥n
Si bien la transcripci√≥n es una herramienta √∫til, en ocasiones puede haber errores o inexactitudes en el texto generado por los modelos de inteligencia artificial. Para evitar esto, nuestra plataforma ofrece la funcionalidad de reproducir la conversaci√≥n original con el usuario. Cada conversaci√≥n cuenta con un bot√≥n de reproducci√≥n en la parte superior derecha. 

Al hacer clic en √©l, se desplegar√° un reproductor de audio en la parte inferior central de la pantalla. Desde all√≠, podr√°s avanzar o retroceder en la conversaci√≥n, pausarla y reanudarla sin ning√∫n problema. La gran ventaja de esta funcionalidad es que puedes tener la conversaci√≥n reproduci√©ndose mientras realizas otras tareas en la plataforma, como revisar la transcripci√≥n o editar la ficha del usuario. De esta manera, tendr√°s acceso a la fuente original y podr√°s asegurarte de que la informaci√≥n que manejas es precisa y completa.

**Implementaci√≥n T√©cnica**
> Cada vez que registramos una conversaci√≥n en nuestra plataforma, ya sea grabada en "real time" o sea porque se ha subido un fichero de audio; nosotros nos guardamos el fichero de audio (el que contiene los propios blobs de audio) en **S3**. <br> <br>
> De esta forma, somos capaces de servir este audio otra vez cuando se solicita. En este caso, cuanod el usuario pulsa el bot√≥n de reproducir una conversaci√≥n, el frontend pide al backend que le devuelva el fichero de audio de esta conversaci√≥n. El backend, simplemente, recupera el fichero de audio de S3 y se lo devuelve al frontend, que es el que se encarga de reproducirlo. En el frontend, hemos utilizado una librer√≠a para hacer el componente, y tambi√©n lo hemos customizado para que tenga el "*look and feel*" de nuestra plataforma. <br> <br>
> Adem√°s, el frontend tambi√©n a√±ade controles sobre este reproductor que hacen mejor la interacci√≥n con el fichero de audio, como tirar hacia delante y atr√°s 5 segundos, hacer pausa o descartar la reproducci√≥n.

##### 9. Ficha de usuario
La ficha de usuario es una caracter√≠stica fundamental de nuestra plataforma, ya que permite recopilar y almacenar informaci√≥n valiosa sobre cada usuario. En esencia, se trata de una lista de preguntas predefinidas que se han dise√±ado para adaptarse a las necesidades espec√≠ficas de nuestro servicio, bas√°ndose en la experiencia previa de la Cruz Roja. A medida que interact√∫as con un usuario a trav√©s de conversaciones en vivo o subiendo conversaciones, la ficha de usuario se actualizar√° autom√°ticamente con la informaci√≥n relevante. Es importante tener en cuenta que las preguntas que ya hayan sido respondidas no ser√°n sobrescritas, pero siempre tendr√°s la opci√≥n de editarlas manualmente si lo necesitas. La ficha de usuario solo se activar√° despu√©s de que se haya tenido al menos una conversaci√≥n con el usuario, y se mostrar√° un icono en la esquina superior derecha del usuario para acceder a ella. En resumen, la ficha de usuario es una herramienta valiosa que te permitir√° recopilar y utilizar informaci√≥n √∫til sobre tus usuarios para mejorar tu experiencia y brindar un mejor servicio.

**Implementaci√≥n T√©cnica**
> En la descripci√≥n de esta funcionalidad nos vamos a centrar en explicar como se guarda y actualizan los datos de la ficha, no en como se generan, que lo explicamos en los puntos 5 y 10. <br> <br>
> Primero de todo, cabe resaltar que, hasta que no se haya tenido m√≠nimo 1 conversaci√≥n con el usuario, no se van a generar las respuestas de este y, por lo tanto, no se va ni a mostrar la opci√≥n de la ficha de usuario en el frontend. <br> <br>
> Una vez ya tenemos datos de qu√© preguntas se han respondio, con que valor se han respondio y cu√°les no lo estan a√∫n, el backend almacena toda esta informaci√≥n en la base de datos y expone dos *endpoints* para que el frontend pueda interaccionar con √©sta. <br> <br>
> El primero es para obetener todos los datos de la ficha de un determinado usuario, d√≥nde el backend simplemente recupera los datos de la base de datos y se los devuelve al frontend en un formato organizado por √°mbitos y indicando cu√°les han sido respondidas y cu√°les no. Cu√°ndo el frontend recibe esta informaci√≥n, lo presenta de una manera agradable y entenedora, d√≥nde cada √°mbito tiene un color e icono diferente, y vemos todos los puntos claramente diferenciados y con la correspondiente respuesta aquellos que ya tienen una. <br> <br>
> Adem√°s, el frontend tambi√©n expone la funcionalidad de editar una respuesta si el voluntario de la Cruz Roja lo cree necesario para hacer algun apunte o modificaci√≥n. Esto simplemente muestra un campo editable y, una vez se pulsa el enter, se env√≠a una petici√≥n al backend para que actualice la respuesta en la base de datos (que √©ste es el segundo *endpoint* que coment√°bamos). <br> <br>

##### 10. Grabar conversaci√≥n
La funcionalidad de grabar en directo es una de las caracter√≠sticas m√°s destacadas de nuestra plataforma y ha requerido de un gran esfuerzo y dedicaci√≥n para conseguir un resultado √≥ptimo. Esta funci√≥n ofrece a los usuarios la posibilidad de grabar una conversaci√≥n en tiempo real directamente desde la plataforma, lo que supone una gran ventaja. Durante la conversaci√≥n, el empleado de Cruz Roja tendr√° en su pantalla una lista de preguntas predefinidas que deber√° realizar al usuario. A medida que se realicen las preguntas, √©stas se marcar√°n en tiempo real para que el empleado pueda seguir un patr√≥n en la entrevista sin perder la atenci√≥n en el usuario.

Una vez finalizada la grabaci√≥n, la conversaci√≥n se procesar√° de la misma manera que cuando se sube una conversaci√≥n ya grabada. En otras palabras, seguir√° el mismo flujo que cuando se sube una conversaci√≥n, lo que permite una gesti√≥n y tratamiento uniforme de la informaci√≥n. En resumen, la funci√≥n de grabaci√≥n en directo es una herramienta valiosa para mejorar la calidad del servicio y asegurarse de que los empleados de Cruz Roja sigan un patr√≥n de entrevista, sin dejar de prestar atenci√≥n a las necesidades del usuario.

**Implementaci√≥n T√©cnica**
> Para poder hacer la implementaci√≥n de las grabaciones de conversaciones en tiempo real y dar un feedback immediato sobre la ficha de usuario, y cumplir con lo quer√≠amos que fuera la experiencia, hemos juntados tres tecnolog√≠as principales. La primera es que hemos utilizado **websockets** para poder tener una comunicaci√≥n en tiempo real entre el backend y el frontend. Cu√°ndo se pulsa el bot√≥n de empezar a grabar, el frontend establece una nueva conexi√≥n de websockets con el backend (que utiliza la librer√≠a de *channels* de Django para poder gestionar los distintos canales que se crean). Este canal se habre y se mantiene abierto hasta que se termina de grabar la conversaci√≥n. Adem√°s, es √∫nico para este usuario (con el que se est√° empezando la conversaci√≥n), y, obviamente, nuestro backend tiene que ser capaz de gestionar varias conexiones a la vez, de conversaciones en directo de m√∫ltiplos usuarios. <br> <br>
> Una vez establecida la conexi√≥n, el frontend empieza a grabar el audio, tomando como fuente de audio el **micr√≥fono del ordenador del empleado**, y pidiendo los permisos para hacerlo (los permisos nativos del navegador). Este audio que captura lo env√≠a cont√≠nuamente (con un "*stream*") al backend. El backend, por su parte, va recibiendo este *stream* de audio y lo que hacemos es transcribirlo en "real time", es decir, que a medida que vamos recibiendo el audio, lo transcribimos y vamos devolviendo la transcripci√≥n al frontend. Para poder hacer esto en tiempo real, tambi√©n hemos tenido que establecer una conexi√≥n en tiempo real con el servicio de transcripci√≥n. <br> <br>
> El frontend, a medida que va recibiendo todo el audio transcrito del backend, lo va almacenando. Esta concatenaci√≥n que vamos haciendo del audio transcrito consideramos que es el contexto de la conversaci√≥n hasta este momento. Recordemos que nuestro objectivo es ser capaces de marcar como "*checked*" los puntos de la ficha que ya se hayan respondido o mencionado durante la conversaci√≥n. Para hacer esto, ahora que ya disponemos del contexto escrito de la conversaci√≥n en tiempo real, el frontend, cada 5 segundos, envia una petici√≥n al backend (en **paralelo y totalmente aparte de la conexi√≥n del websocket** que ya tiene establecida para seguir transcirbiendo). En esta petici√≥n le pasa el contexto que tiene hasta este momento, y le pide que le indique de qu√© se ha hablado ya, para poder marcarlo en concordancia en la interf√≠cie gr√°fica. <br> <br>
> El backend, cuando recibe estas peticiones cada 5 segundos, utiliza la tecnologia de **ChatGPT** que ya hemos mencionado antes para **preguntar si ya se ha hablado de los puntos que faltan por hablar** (pasando como contexto al chat lo que recibe del frontend, y las pregutnas o puntos, que ya dispone de ellos en la base de datos). Hemos dedicado tiempo a refinar i perfeccionar el "*prompt*" que le enviamos al ChatGPT para que su respuesta sea en un formato que a nostros nos sea f√°cil marcalo como "*checked*" o no. <br> <br>
> Una vez el backend recibe la respuesta del ChatGPT, se la devuelve al frontend en un formato adecuado, que es el que se encarga de marcar los puntos que ya se han hablado. <br> <br>
> Cabe destacar que, para optimizar las llamada que hacemos a ChatGPT, no le pasamos siempre todas las preguntas, sino las que **faltan a√∫n por responder** y ya est√°.

##### 11. Buscador
La funcionalidad del buscador es una herramienta muy √∫til en nuestra plataforma, ya que nos permite encontrar r√°pidamente cualquier conversaci√≥n o usuario de Cruz Roja. Sabemos que la Cruz Roja tiene un gran n√∫mero de usuarios y consultas, por lo que hemos utilizado la tecnolog√≠a Opensearch para implementar un buscador gen√©rico en la plataforma.

El buscador se encuentra en la parte superior derecha de la plataforma y, al pulsar el icono de b√∫squeda, se abrir√° una ventana emergente con un campo de entrada que nos permitir√° introducir una palabra clave y buscar entre todas las conversaciones de los empleados. Al pulsar una conversaci√≥n del buscador, seremos redirigidos al usuario correspondiente y la conversaci√≥n se mostrar√° en la parte superior de la p√°gina. Es importante tener en cuenta que, si refrescamos la p√°gina, seguiremos con el m√©todo de ordenaci√≥n normal.

En resumen, la funcionalidad del buscador es una herramienta muy √∫til y eficaz que nos permite encontrar r√°pidamente cualquier conversaci√≥n o usuario en la plataforma de Cruz Roja, lo que nos ayuda a ahorrar tiempo y a mejorar la experiencia del usuario.

**Implementaci√≥n T√©cnica**
> Para poder tener b√∫squedas eficientes y r√°pidas en nuestra plataforma, hemos utilizado el motor de b√∫squeda AWS OpenSearch. Incluimos esta tecnolog√≠a porque sab√≠amos que dispondr√≠amos de much√≠smios datos (de texto) dado que vamos a transcribir cada conversaci√≥n que se haga en la plataforma. Por eso, hemos utilizado AWS OpenSearch para poder indexar todos los datos de texto que tenemos en nuestra base de datos, y poder hacer b√∫squedas eficientes y r√°pidas. <br> <br>
> Para indexar todas las conversaciones, tenemos un **Django Command** que indexa todas las conversaciones que tenemos en la base de datos. Este comando nos permite que no tengamos que guardar y hacer backups tan estrictamente, ya que se pueden "reconstruir" todos a partir de la informaci√≥n que tenemos en la base de datos. Este comando es √∫til para crear des de 0 los datos de OpenSearch. No obstante, para tener una indexaci√≥n cont√≠nua, lo que hemos hecho es que cada vez que se crea una conversaci√≥n en nuestra plataforma, la indexamos autom√°ticamente en OpenSearch. <br> <br>
> Por √∫ltimo, el backend expone un endpoint de b√∫squeda de texto libre al frontend, para que se pueda buscar lo que se introduce en el buscador. El backend hacer de **proxy** para poder interactuar con el OpenSearch, ya que √©ste no es p√∫blico. El backend devuelve los resultados que de el OpenSearch, y tambi√©n marca con *tags* el **highlighting** del t√©rmino que ha encontrado. El frontend, cuando lo recibe, lo muestra de una manera f√°cil al usuario, y resaltado en negrita el t√©rmino que ha buscado (que ya le viene marcado por el backend).

##### 12. Ver notificaciones
La funcionalidad de las notificaciones nos permite mantener a los usuarios informados en tiempo real sobre el estado de sus conversaciones. Como mencionamos anteriormente, todos los procesos en nuestra plataforma son as√≠ncronos, por lo que es importante notificar al usuario en cuanto la conversaci√≥n ha sido procesada.

Cuando una conversaci√≥n es procesada exitosamente, adem√°s del mensaje de √©xito que se muestra en la pantalla, tambi√©n aparecer√° una notificaci√≥n en el icono de la campana en la parte superior derecha de la p√°gina. Esto permite al usuario ver f√°cilmente el estado de sus conversaciones y ser notificado en tiempo real cuando una conversaci√≥n ha sido procesada.

Adem√°s, la funcionalidad de las notificaciones tambi√©n nos permite acceder r√°pidamente a las conversaciones procesadas. Simplemente pulsando en la notificaci√≥n correspondiente, seremos redirigidos a la conversaci√≥n procesada y podremos continuar con nuestro trabajo.

**Implementaci√≥n T√©cnica**
> Para poder tener estas notificaciones, lo hemos hecho b√°sicamente des de la parte del frontend. √âste es el que va enviando las peticiones y, como nuestra aplicaci√≥n es as√≠ncrona, el usuario puede hacer otras cosas mientras se procesan. <br> <br>
> Una vez el frontend recibe el resultado del backend, lo que hace es mostrar una model de que se ha completado y se a√±ade una notificaci√≥n en la campana (usando el **local storage** para guardar este estado). Cu√°ndo el usuario pulsa sobre la notificaci√≥n, el frontend es capaz de redirigirlo al sitio correspondiente dentro de la plataforma de d√≥nde esta notificaci√≥n hace referencia. <br> <br>
> Una vez ya se ha visto una notificaci√≥n, esta se elimina del apartado de notificaciones y del local storage.

### Arquitectura Cloud

#### Descripci√≥n

Link video

#### Repositorios

- [Terraform](https://github.com/JPG-squad/terraform)

- Utilizamos **aws-vault** para ejecutar Terraform de forma segura y con una capa adicional de seguridad mediante MFA (autenticaci√≥n multifactor).  AWS-Vault tambi√©n ofrece la opci√≥n de guardar las credenciales cifradas en el disco.

- Usamos **Terraform Workspaces** para mantener el mismo c√≥digo de Terraform y poder crear los mismos recursos en m√∫ltiples regiones, como Espa√±a e Irlanda. Al usar workspaces, podemos tener diferentes entornos de infraestructura (por ejemplo, default y drp) en diferentes regiones y a√∫n as√≠ mantener el mismo c√≥digo de Terraform. Esto nos ayuda a mantener un proceso de implementaci√≥n consistente y predecible en todas nuestras regiones. Adem√°s, nos permite realizar cambios en un workspace sin afectar a los dem√°s, lo que mejora la gesti√≥n de cambios en nuestras infraestructuras.

[![Terraform Icon](https://img.shields.io/badge/-Terraform-purple?logo=terraform&logoColor=white&style=flat)](https://www.terraform.io/)
[![aws-vault Icon](https://img.shields.io/badge/-aws--vault-orange?logo=amazon-aws&logoColor=white&style=flat)](https://github.com/99designs/aws-vault)

#### AWS Services

##### EC2, ECS, ECR, Fargate, ALB

Utilizamos Amazon EC2 por dos razones principales en nuestra infraestructura. En primer lugar, aprovechamos EC2 (concretamente instancias con los procesadores **gr√°viton**) para provisionar la capacidad de nuestras tareas de backend de ECS a un costo menor que Fargate en nuestra cuenta de producci√≥n. Esto nos permite optimizar nuestros costos mientras seguimos manteniendo la capacidad de controlar la capacidad de nuestras tareas de backend. Adem√°s, utilizamos EC2 para lanzar un Bastion Host cuando es necesario, que se puede detener f√°cilmente cuando no se necesita. El Bastion Host sirve como un proxy SSH seguro que nos permite acceder a nuestros paneles de OpenSearch de manera segura.

Utilizamos Amazon ECS como nuestro orquestador de servicios para administrar y orquestar nuestros servicios contenerizados en la nube. Tambi√©n utilizamos Fargate, que nos permite ejecutar contenedores de Docker sin tener que administrar las instancias subyacentes de EC2. En particular, usamos Fargate para iniciar nuestro servicio de backend DRP (en Irlanda).

Finalmente, utilizamos el servicio de balanceo de carga de aplicaciones (ALB) para enrutar las solicitudes entrantes a trav√©s de nuestros servicios contenerizados.

Ficheros de terraform:
- [Carpeta configuraci√≥n Cluster ECS, EC2 instances used as infrastructure for ECS with autoscaling and Bastion Host](https://github.com/JPG-squad/terraform/tree/main/services/ecs/cluster)
- [Carpeta configuraci√≥n application load balancer](https://github.com/JPG-squad/terraform/tree/main/services/alb/public)
- [Carpeta configuraci√≥n servicio backend, contiene ECR, especificaciones de corret EC2 or Fargate on DRP](https://github.com/JPG-squad/terraform/tree/main/applications/backend)

##### Transcribe

Utilizamos AWS Transcribe para obtener transcripciones precisas y diarizadas de nuestras conversaciones y entrevistas. La funci√≥n de diarizaci√≥n nos permite identificar qu√© persona est√° hablando en cada momento, lo que facilita el an√°lisis de la conversaci√≥n y nos permite obtener informaci√≥n relevante para nuestra aplicaci√≥n. Esta herramienta es muy √∫til para nosotros, ya que nos ahorra tiempo y esfuerzo al transcribir nuestras conversaciones de manera autom√°tica y precisa.

##### Aurora RDS

Utilizamos AWS Aurora como nuestra base de datos relacional para nuestra aplicaci√≥n. Esta herramienta nos permite tener una base de datos altamente disponible en m√°s de una zona de disponibilidad, lo que garantiza que nuestros datos siempre est√©n disponibles. Adem√°s, no tenemos que preocuparnos por la cantidad de espacio libre en nuestra base de datos, ya que AWS Aurora se encarga autom√°ticamente de escalar nuestra base de datos para satisfacer nuestras necesidades de almacenamiento en funci√≥n de la demanda. Esto nos permite enfocarnos en nuestro negocio principal sin tener que preocuparnos por mantener y administrar nuestra base de datos.

Ficheros de terraform:
- [Carpeta configuraci√≥n RDS](https://github.com/JPG-squad/terraform/tree/main/services/rds/postgresql)

##### Opensearch

Utilizamos AWS OpenSearch como nuestro servicio administrado de b√∫squeda en nuestro negocio. Esta herramienta nos permite ahorrar tiempo y esfuerzo al no tener que preocuparnos por la gesti√≥n y el mantenimiento de la infraestructura de b√∫squeda. Con AWS OpenSearch, podemos almacenar nuestras conversaciones en formato JSON y buscar texto en ellas de forma r√°pida y sencilla. Adem√°s, AWS OpenSearch es altamente escalable, los datos de b√∫squeda se almacenan en cl√∫steres de datos altamente disponibles y se cifran tanto en tr√°nsito como en reposo, lo que garantiza la privacidad y la protecci√≥n de los datos de los clientes.

Ficheros de terraform:
- [Carpeta configuraci√≥n del dominio de Opensearch](https://github.com/JPG-squad/terraform/tree/main/services/opensearch)

##### IAM

El contenedor de backend asume un rol que tiene permisos espec√≠ficos para leer y escribir en el bucket de backend de AWS, as√≠ como para ejecutar trabajos de transcripci√≥n y visualizar los resultados correspondientes.

Adem√°s, para el desarrollo local, contamos con unas credenciales compartidas que tienen acceso a un bucket de S3 de desarrollo que no contiene datos sensibles. Estas credenciales solo nos permiten trabajar con videos y resultados de prueba generados por Transcribe, pero no nos dan acceso directo a la herramienta de transcripci√≥n. Por seguridad, rotamos estas credenciales cada 3 meses como pol√≠tica est√°ndar.

Finalmente, para el uso de Terraform, utilizamos aws-vault, lo que nos permite asumir un rol temporal de una hora gracias al servicio de Security Token Service (STS). Estas credenciales ef√≠meras se ejecutan en una subshell que se encarga de ejecutar Terraform, lo que garantiza que el acceso a los recursos de AWS se maneje de manera segura y controlada.

![Arquitectura local](img/IAM.svg)

Ficheros de terraform:
- [Carpeta principal de configuraci√≥n IAM](https://github.com/JPG-squad/terraform/tree/main/global/iam)
- [IAM roles y policies del servico de backend para pull & push on ECR y para desplegar el servicio](https://github.com/JPG-squad/terraform/blob/main/applications/backend/iam.tf)

##### S3

Utilizamos Amazon S3 para varias tareas en nuestra aplicaci√≥n. En primer lugar, alojamos nuestro frontend web en la regi√≥n de Espa√±a e Irlanda. Adem√°s, utilizamos S3 para almacenar los audios de las conversaciones y los archivos de salida generados por AWS Transcribe en la regi√≥n de Par√≠s. Tambi√©n replicamos estos archivos a un bucket en la regi√≥n de Irlanda para tener redundancia y garantizar la disponibilidad de los datos en caso de cualquier interrupci√≥n en una regi√≥n determinada. La flexibilidad y escalabilidad de S3 nos permite gestionar f√°cilmente nuestro almacenamiento en la nube de manera eficiente y sin preocupaciones.

Tambi√©n aplicamos pol√≠ticas de ciclo de vida de S3 en el bucket de audios. Con estas pol√≠ticas, podemos automatizar la transici√≥n de los objetos a diferentes clases de almacenamiento, como a clase de acceso menos frecuente (S3 Standard-Infrequent Access) o almacenamiento en fr√≠o (S3 Glacier), despu√©s de un cierto per√≠odo de tiempo. 

![Ciclo de vida bucket backend](img/backend_lifecycle.png)

Ficheros de terraform:
- [Carpeta de configuraci√≥n de S3](https://github.com/JPG-squad/terraform/tree/main/services/s3)

##### Cloudfront

Utilizamos Amazon CloudFront para distribuir el contenido de nuestro frontend web de manera global y asegurar una baja latencia y alta disponibilidad. Usamos dos buckets de Amazon S3: uno configurado como primario y otro como secundario en caso de desastres. Tambi√©n nos permite definir reglas para cachear y entregar contenido est√°tico Adem√°s, CloudFront tiene la capacidad de utilizar certificados SSL personalizados para proporcionar una conexi√≥n segura a nuestro sitio web.

Ficheros de terraform:
- [Carpeta de configuraci√≥n de cloudfront](https://github.com/JPG-squad/terraform/tree/main/services/cloudfront/frontend)

##### Cloudwatch, SNS

Utilizamos Amazon CloudWatch para monitorear nuestros servicios en la nube y obtener informaci√≥n sobre el rendimiento de nuestra aplicaci√≥n. Con CloudWatch, podemos configurar alarmas y reaccionar autom√°ticamente a los cambios en nuestros recursos. Actualmente enviamos estas alarmas a un canal de Slack, utilitzando SNS a traves del email del canal.

Tenemos el siguiente dashboard con las m√©tricas m√°s importantes de nuestra aplicaci√≥n, en la [siguiente carpeta dentro de este repostorio](monitor/) tenemos 3 images de como se ve el dashboard.

Ejemplo de alertas:

![Alertas](img/alertas.png)


Y las alarmas que tenemos configuradas son las siguientes:

1. Alarma para la utilizaci√≥n de CPU de ECS: esta alarma se activa cuando la utilizaci√≥n de la CPU del servicio ECS supera el 80%.

2. Alarma para la utilizaci√≥n de memoria de ECS: esta alarma se activa cuando la utilizaci√≥n de la memoria del servicio ECS supera el 80%. 

3. Alarma para la utilizaci√≥n de CPU de RDS: esta alarma se activa cuando la utilizaci√≥n de la CPU de una instancia de base de datos RDS supera el 80%. 

4. Alarma para la memoria libre de RDS: esta alarma se activa cuando el espacio de almacenamiento disponible en una instancia de base de datos RDS cae por debajo de 1 GB.

5. Alarma para la cantidad de hosts saludables: esta alarma se activa cuando la cantidad de hosts saludables de un grupo de destino de Elastic Load Balancer cae por debajo de 1. Esto puede indicar que uno o m√°s hosts han fallado y que se debe tomar medidas para solucionar el problema.

6. Alarma para el espacio de almacenamiento libre de Elasticsearch: esta alarma se activa cuando el espacio de almacenamiento libre en un dominio de Elasticsearch cae por debajo de 5 GB.

7. Alarma para la cantidad de trabajos de transcripci√≥n ejecutados en los √∫ltimos 30 minutos: esta alarma se activa cuando se han ejecutado m√°s de 30 trabajos de transcripci√≥n en los √∫ltimos 30 minutos.

Ficheros de terraform:
- [Carpeta de configuraci√≥n de cloudwatch y sns](https://github.com/JPG-squad/terraform/blob/main/global/monitor)

##### ACM

Utilizamos ACM para crear certificados SSL/TLS para dos dominios distintos. El primer dominio es https://portal-conversaciones.app-deploy.com, el cual es utilizado en las regiones de Espa√±a e Irlanda, y el segundo dominio es https://app.portal-conversaciones.app-deploy.com, el cual se utiliza en la regi√≥n de Virginia.

Ficheros de terraform:
- [Carpeta de configuraci√≥n de ACM](https://github.com/JPG-squad/terraform/tree/main/services/acm)
##### Route53

AWS Route53 es un servicio de DNS administrado que utilizamos para crear y administrar los dominios necesarios para nuestra aplicaci√≥n. Adem√°s, tambi√©n utilizamos Route53 para crear health checks con failover en cada una de nuestras regiones. Estos health checks se utilizan para monitorear el estado de nuestro backend en cada regi√≥n y asegurarnos de que est√° disponible y respondiendo correctamente. Si se detecta un problema en una regi√≥n, Route53 puede redirigir autom√°ticamente el tr√°fico a otra regi√≥n que est√© disponible y funcione correctamente.

Ficheros de terraform:
- [Carpeta de configuraci√≥n de route53](https://github.com/JPG-squad/terraform/tree/main/global/route53)
- [ALB route53](https://github.com/JPG-squad/terraform/tree/main/services/alb/public/route53.tf)
- [Cloudfront route53](https://github.com/JPG-squad/terraform/blob/main/services/cloudfront/frontend/main.tf)

##### Backups

Utilizamos AWS Backups para asegurarnos de que nuestra base de datos RDS Aurora principal de producci√≥n est√© respaldada regularmente de acuerdo con nuestra pol√≠tica de backups. Adem√°s, para garantizar la disponibilidad y la recuperaci√≥n en caso de un desastre, tambi√©n replicamos estos backups a la regi√≥n de Irlanda, lo que nos permite recuperar nuestra base de datos en caso de un fallo o interrupci√≥n en la regi√≥n principal.

Ficheros de terraform:
- [Carpeta de configuraci√≥n de backups](https://github.com/JPG-squad/terraform/tree/main/services/backups)

##### Config

Config es un servicio que utilizamos en nuestra arquitectura para hacer seguimiento de los cambios realizados en los recursos de AWS en diferentes regiones. Lo usamos en Espa√±a, Par√≠s, Irlanda y Virginia para tener una vista completa y detallada del estado y la configuraci√≥n de los recursos de AWS.

Ficheros de terraform:
- [Carpeta de configuraci√≥n de config](https://github.com/JPG-squad/terraform/tree/main/services/config)

##### Cloudtrail

En nuestra arquitectura, hemos utilizado CloudTrail para hacer un seguimiento de las acciones realizadas en nuestra infraestructura de AWS. La raz√≥n principal para utilizar CloudTrail es para poder auditar y monitorear las acciones realizadas en nuestra cuenta de AWS, lo que nos ayuda a detectar y prevenir posibles problemas de seguridad.

En particular, hemos habilitado CloudTrail en Virginia con la opci√≥n de multiregi√≥n, lo que nos permite hacer un seguimiento de las acciones realizadas en todas las regiones donde tenemos recursos de AWS. De esta manera, podemos tener una visibilidad completa de todas las acciones realizadas en nuestra infraestructura de AWS, incluso en regiones donde no se han habilitado CloudTrail de forma individual.

Ficheros de terraform:
- [Carpeta de configuraci√≥n de cloudtrail](https://github.com/JPG-squad/terraform/tree/main/services/cloudtrail)

#### Recuperaci√≥n de desastres

Este es nuestro plan de recuperaci√≥n de desastres para garantizar la disponibilidad y continuidad de nuestros servicios en caso de cualquier imprevisto. El plan consiste en las siguientes acciones:

1. **Backups de base de datos**: Utilizamos el servicio AWS Backups para realizar copias de seguridad de nuestra base de datos y las replicamos a la regi√≥n de Irlanda.

2. **Im√°genes del backend en ECR**: Subimos las im√°genes del backend tanto en el ECR de Espa√±a como en el de Irlanda.

3. **Frontend en buckets de S3**: El frontend (web) est√° alojado en buckets de S3 en Espa√±a e Irlanda. Al subir el c√≥digo se hace en ambos buckets a la vez, y nuestra CDN Cloudfront tiene configurado el bucket de Espa√±a como principal y el de Irlanda como failover.

4. **Bucket S3 en Par√≠s**: En el servicio de producci√≥n, usamos Transcribe en Par√≠s y por eso necesitamos un bucket de S3 en esa regi√≥n. Este bucket lo replicamos usando la funcionalidad de S3 a uno igual en la regi√≥n de Irlanda.

5. **Cluster Fargate en Irlanda**: En Irlanda, configuramos un cluster Fargate con las mismas caracter√≠sticas que el de Espa√±a, pero sin EC2. El servicio tiene asociado 0 tareas corriendo. Utilizamos healthchecks de Route53 para comprobar si los servicios de las dos regiones est√°n healthy pasando por los ALB, y dependiendo de cu√°l est√© healthy, hacemos failover a Irlanda.

6. **Ejecuci√≥n del script de DRP**: Cuando hay un desastre, ejecutamos un script manual que crea una base de datos a partir del backup de la de producci√≥n. Una vez que la base de datos est√° creada, actualizamos el servicio de Irlanda corriendo en Fargate, que `desired_count = 1`.

Este modelo de recuperaci√≥n de desastres se adapta a nuestras necesidades y presupuesto, al mismo tiempo que nos permite cumplir con los pilares de sustentabilidad y optimizaci√≥n de costos. Adem√°s, hemos realizado pruebas para asegurarnos de que el plan es efectivo y ha sido exitoso en nuestras √∫ltimas pruebas.

Adjunto encontrar√°s un video de nuestra √∫ltima prueba de DRP.
<a href="https://www.youtube.com/watch?v=TUZmK0X9zgk">Demostraci√≥n del Plan de Recuperaci√≥n de Desastres</a>

##### Politica de backups

Nuestra pol√≠tica de backups es la siguiente:
1. Realizamos backups de la base de datos cada 1 hora los cuales expiran despu√©s de 30 d√≠as.
2. Realizamos backups de la base de datos cada 7 d√≠as los cuales expiran despu√©s de 90 d√≠as.
3. Realizamos backups de la base de datos cada 30 d√≠as los cuales expiran despu√©s 1 a√±o.

Ficheros de terraform:
- [Politicas de backups](https://github.com/JPG-squad/terraform/blob/main/services/backups/main.tf)

##### DRP Test

Hemos realizado una prueba de verificaci√≥n para comprobar que podemos recuperarnos en otra regi√≥n en caso de que ocurra alg√∫n desastre en la regi√≥n de Espa√±a. El tiempo total de recuperaci√≥n ha sido de 15 minutos, lo que es muy bueno para una arquitectura failover.

Adjunto se encuentra el enlace al video de la prueba de desastre:

<a href="https://youtu.be/Jw4I-6NhjU4">DRP Test - Portal de conversaciones - AWS Hackathon for good 2023</a>


##### MTTR ( Mean Time To Recover )

Nuestra media de tiempo de recuperaci√≥n hasta el momento es de 15 minutos. Es importante tener en cuenta que contamos con backups cada hora y replicados en otra regi√≥n, lo que significa que asumimos que como m√°ximo podr√≠amos perder datos correspondientes a una hora. Esto es aceptable para nosotros, ya que no tenemos datos cr√≠ticos que no podamos perder.

#### AWS Pilars

##### Excelencia operativa

1. **Realizar operaciones como c√≥digo**: Utilizamos Terraform para desplegar nuestra infraestructura como c√≥digo. Esto nos permite tener una infraestructura versionada y repetible. El repositorio se encuentra [aqu√≠](https://github.com/JPG-squad/terraform)
2. **Realizar cambios frecuentes, peque√±os y reversibles**: Hemos establecido un proceso para desplegar cambios en peque√±as incrementos que se pueden revertir r√°pidamente si surgen problemas, de esta manera podemos identificar f√°cilmente la causa ra√≠z de cualquier problema y minimizar el tiempo y los recursos necesarios para solucionarlos.
3. **Anticiparse al fallo**: Hemos estado haciendo pruebas DRP para asegurarnos de que podemos recuperarnos de cualquier fallo. Tambi√©n hemos implementado alarmas y notificaciones para estar al tanto de cualquier problema que pueda surgir. En el siguiente c√≥digo de Terraform (https://github.com/JPG-squad/terraform/blob/main/global/monitor/alarms.tf) se puede ver c√≥mo hemos implementado las alarmas y tambi√©n [aqu√≠](https://youtu.be/Jw4I-6NhjU4) se puede ver un v√≠deo con la √∫ltima prueba DRP que realizamos.

##### Seguridad

1. **Implementar una s√≥lida base de identidad**: Utilizamos AWS IAM para gestionar el acceso a los servicios y recursos de AWS de forma segura.
2. **Mantener una configuraci√≥n segura**: Utilizamos AWS Config para supervisar nuestra infraestructura y asegurarnos de que cumpla con nuestras pol√≠ticas de seguridad.
3. **Aplicar seguridad en todas las capas**: Utilizamos AWS WAF para proteger nuestra aplicaci√≥n de exploits web comunes que podr√≠an afectar la disponibilidad de la aplicaci√≥n, comprometer la seguridad o consumir recursos excesivos. Tambi√©n usamos grupos de seguridad para controlar el tr√°fico que se permite llegar a cada servicio, por ejemplo, solo permitimos el tr√°fico desde el balanceador de carga a los servicios backend.
4. **Proteger los datos en tr√°nsito y en reposo**: Utilizamos AWS KMS para gestionar las claves de cifrado que se utilizan para cifrar nuestros datos. Tambi√©n usamos AWS Secrets Manager para los secretos utilizados por nuestros servicios. Los buckets de S3 se cifran por defecto.
5. Prepararse para eventos de seguridad: POR HACER.

##### Fiabilidad

1. **Recuperaci√≥n autom√°tica ante fallos, escalar horizontalmente para aumentar la disponibilidad de carga de trabajo agregada y dejar de adivinar la capacidad**: Para garantizar una alta disponibilidad y resiliencia, utilizamos AWS Auto Scaling para ajustar la capacidad, AWS Load Balancing para distribuir el tr√°fico y AWS RDS Aurora para una base de datos altamente disponible. Adem√°s, el servicio Opensearch se implementa en dos zonas de disponibilidad para obtener redundancia y tolerancia a fallos adicionales. Nuestros servicios se implementan en dos zonas de disponibilidad para obtener redundancia y tolerancia a fallos adicionales. Tambi√©n contamos con un plan DRP para recuperarnos de cualquier fallo.
2. **Probar los procedimientos de recuperaci√≥n**: Hemos estado haciendo pruebas DRP para asegurarnos de que podemos recuperarnos de cualquier fallo. [Aqu√≠](https://youtu.be/Jw4I-6NhjU4) se puede ver un v√≠deo con la √∫ltima prueba DRP que realizamos.
3. **Gestionar el cambio en la automatizaci√≥n**: Utilizamos Terraform para desplegar nuestra infraestructura como c√≥digo. Esto nos permite tener una infraestructura versionada y repetible. El repositorio se encuentra [[aqu√≠](https://github.com/JPG-squad/terraform). Sin embargo, actualmente Terraform no se aplica autom√°ticamente, pero esto se puede hacer en el futuro si el equipo crece.

##### Eficiencia del rendimiento

1. **Democratizar tecnolog√≠as avanzadas**: Estamos democratizando tecnolog√≠as avanzadas mediante el uso de servicios de AWS como ECS, RDS Aurora, OpenSearch, Transcribe, Route53, CloudWatch, S3, ALB y otros. Este enfoque nos permite aprovechar tecnolog√≠as como bases de datos SQL, transcodificaci√≥n de medios y aprendizaje autom√°tico sin requerir experiencia especializada de nuestro equipo de TI. Tambi√©n nos permite centrarnos en el desarrollo de productos en lugar de la aprovisionamiento y gesti√≥n de recursos.
2. **Globalizarse en minutos**: Usamos AWS CloudFront para entregar contenido a nuestros usuarios con baja latencia y alta velocidad de transferencia. Tambi√©n usamos AWS Route53 para dirigir el tr√°fico a la regi√≥n AWS m√°s cercana. Adem√°s, estamos ajustando autom√°ticamente la capacidad de nuestros servicios.
3. **Usar arquitecturas sin servidor**: Solo usamos S3, Transcribe, SNS, Fargate (DRP). Y la raz√≥n principal es porque queremos tener una arquitectura sin servidor para evitar la necesidad de gestionar servidores y tambi√©n reducir costos. Sin embargo, estamos utilizando ECS en EC2 para implementar los servicios backend, pero esto se debe a que queremos optimizar costos, ya que Fargate es m√°s caro que ECS.
4. **Considerar la simpat√≠a mec√°nica**: Para garantizar un acceso eficiente a los datos y un rendimiento √≥ptimo, hemos elegido utilizar OpenSearch para buscar en un gran volumen de conversaciones y PostgreSQL para la l√≥gica central de la plataforma. Al seleccionar estas tecnolog√≠as, podemos lograr escalabilidad, confiabilidad y rentabilidad en nuestra arquitectura.

##### Optimizaci√≥n de costos

Adem√°s de los puntos com√∫nmente explicados en el pilar de Optimizaci√≥n de costos, queremos hacer √©nfasis en que hemos elegido utilizar AWS EC2 on EC2 como soluci√≥n para nuestra infraestructura de backend. Esto se debe a que las instancias EC2 son m√°s econ√≥micas en comparaci√≥n con Fargate, y en un futuro el ahorro de costos reservando instancias EC2 puede ser a√∫n mayor. Al seleccionar una soluci√≥n de infraestructura m√°s econ√≥mica, podemos optimizar nuestros costos y asignar nuestros recursos a otros aspectos cr√≠ticos del negocio.


1. **Implementar gesti√≥n financiera en la nube**: Utilizamos AWS Cost Explorer para visualizar, entender y gestionar nuestros costos y uso de AWS con el tiempo. Tambi√©n podr√≠amos usar AWS Budgets para establecer presupuestos personalizados que nos alerten cuando nuestros costos o uso excedan (o se pronostiquen que excedan) nuestro monto presupuestado, pero esto a√∫n no est√° implementado.
2. **Adoptar un modelo de consumo**: En primer lugar, hemos detenido la infraestructura EC2 no utilizada que no era necesaria durante las horas de menor actividad o cuando no se necesitaba realizar pruebas. Adem√°s, durante la fase de desarrollo, hemos utilizado solo una instancia en lugar de m√∫ltiples instancias en m√∫ltiples zonas de disponibilidad. Para el per√≠odo de evaluaci√≥n, tambi√©n hemos utilizado solo una instancia para minimizar los costos. En segundo lugar, hemos utilizado AWS RDS con un nivel gratuito ya que la carga de trabajo era suficiente. De manera similar, hemos utilizado OpenSearch con solo un nodo y un nivel gratuito. Este enfoque nos ha ayudado a evitar el consumo innecesario de recursos y nos ha permitido permanecer dentro de nuestro presupuesto limitado de 50 euros. Al adoptar un modelo de consumo, nos hemos asegurado de que solo pagamos por los recursos inform√°ticos que necesitamos y podemos aumentar o disminuir f√°cilmente el uso seg√∫n nuestros requisitos empresariales. Esto nos ha ayudado a lograr ahorros significativos de costos mientras mantenemos un alto nivel de rendimiento y confiabilidad en nuestra infraestructura en la nube.
3. **Medir la eficiencia general**: Usando los paneles de CloudWatch, podemos medir la eficiencia general de nuestra infraestructura. Podemos ver el uso de la CPU, el uso de la memoria, el espacio libre restante, la salud de los servicios, etc. Adem√°s, tenemos alarmas de CloudWatch para estar al tanto de cualquier problema que pueda surgir.

##### Sostenibilidad

1. **Maximizar la utilizaci√≥n**: Actualmente, estamos cumpliendo con la regla de maximizar la utilizaci√≥n mediante el uso de una sola instancia y la selecci√≥n del tama√±o de instancia m√°s peque√±o para cada servicio debido a limitaciones presupuestarias. Sin embargo, hemos tomado medidas para analizar la utilizaci√≥n utilizando el panel de CloudWatch y alarmas, y estamos preparados para escalar seg√∫n sea necesario para aumentar la utilizaci√≥n y maximizar la eficiencia energ√©tica del hardware subyacente.
2. **Anticipar y adoptar nuevas ofertas de hardware y software m√°s eficientes**: Estamos utilizando los nuevos procesadores AWS Graviton dise√±ados para ofrecer la mejor relaci√≥n precio-rendimiento para nuestras cargas de trabajo en la nube que se ejecutan en Amazon EC2 con arm. Esto nos permite aprovechar los √∫ltimos avances tecnol√≥gicos y mejorar la eficiencia de nuestras cargas de trabajo en la nube.
3. **Usar servicios administrados**: El uso de servicios administrados como AWS Fargate y las configuraciones de ciclo de vida de Amazon S3 nos permite compartir recursos en una amplia base de clientes.

#### CI/CD

![CI/CD](img/CI_CD.svg)

#### Costes


En la secci√≥n de costes, se utiliz√≥ la herramienta AWS Calculator para generar dos informes de los costes aproximados de nuestra infraestructura. Se realiz√≥ un c√°lculo para la infraestructura que est√° corriendo en el [siguiente enlace del PDF](files/dev_calc.pdf), y otro para la infraestructura que usar√≠amos en producci√≥n, que se puede encontrar en [este enlace del PDF](files/prod_calc.pdf).

En ambos informes, se incluyeron los costes fijos y algunas aproximaciones de uso, como el n√∫mero de trabajos ejecutados de Transcribe. Es importante tener en cuenta que estos costes son solo una estimaci√≥n y pueden variar en funci√≥n del uso real de la infraestructura. Por lo tanto, se recomienda revisar los informes peri√≥dicamente y ajustarlos seg√∫n sea necesario para mantener los costes dentro del presupuesto establecido.

Adem√°s, es importante destacar que hasta el momento solo hemos gastado 24 euros gracias al uso del free tier que ofrece Amazon en diferentes servicios y a la optimizaci√≥n de costes y sostenibilidad en nuestra infraestructura. Esto se debe a que hemos utilizado solo lo que necesit√°bamos y no hemos sobredimensionado la infraestructura, siguiendo as√≠ el pilar de optimizaci√≥n de costes y sostenibilidad en nuestra estrategia de gesti√≥n de infraestructura en la nube.

![Costes actuales](img/currentcosts.png)

## ‚è≠Ô∏è Local development

Nuestra arquitectura local es una r√©plica de la que utilizamos en la nube, gracias al uso de Docker y Docker Compose. De esta manera, hemos conseguido una arquitectura port√°til para nuestros desarrolladores. Es importante destacar que las credenciales de desarrollo en local solo tienen permisos para leer y escribir en S3, lo cual es suficiente ya que contamos con ficheros de ejemplo de transcripci√≥n en S3. Por lo tanto, no es necesario otorgar permisos de transcripci√≥n, lo que podr√≠a resultar peligroso si las credenciales se perdieran o si un desarrollador cometiera un error y ejecutara muchos trabajos a la vez.

![Arquitectura local](img/localarchitecture.svg)

## GDPR

En una hacktoon, no podemos abordar todos los aspectos de la GDPR en detalle, pero s√≠ podemos dar una visi√≥n general de los puntos m√°s importantes y c√≥mo nuestra aplicaci√≥n cumple con ellos, as√≠ como se√±alar √°reas que pueden mejorarse en el futuro.

1. Obtener el consentimiento del usuario: Es importante que el empleado de la Cruz Roja explique c√≥mo se procesar√°n los datos del usuario y obtenga su consentimiento antes de recopilar cualquier informaci√≥n o grabar conversaciones. Nuestra aplicaci√≥n tambi√©n deber√≠a permitir que los usuarios retiren su consentimiento en cualquier momento.

2. Minimizar la recopilaci√≥n de datos: Es crucial definir qu√© informaci√≥n necesitamos de los usuarios y minimizar la cantidad de datos personales que se recopilan.

3. Garantizar la seguridad de los datos: Nuestra aplicaci√≥n utiliza medidas de seguridad para proteger la informaci√≥n personal de los usuarios, como el cifrado de datos y la autenticaci√≥n de usuarios autorizados.

4. Garantizar el acceso y la portabilidad de los datos: Actualmente, solo los empleados de la Cruz Roja tienen acceso a los datos, pero se deber√≠a implementar un sistema para que los usuarios puedan acceder y ver sus propios datos. Tambi√©n se podr√≠a facilitar la transferencia de datos.
  
5. Proporcionar una forma f√°cil de eliminar los datos: Si bien actualmente no existe esta funcionalidad, se podr√≠a implementar en el futuro.

6. Proporcionar una pol√≠tica de privacidad clara y f√°cil de entender: Es esencial que nuestra aplicaci√≥n cuente con una pol√≠tica de privacidad que explique claramente c√≥mo se recopila, utiliza y comparte la informaci√≥n personal del usuario.

7. Proporcionar informaci√≥n clara sobre c√≥mo se utiliza la informaci√≥n recopilada: Aunque en esta documentaci√≥n se ha explicado c√≥mo se utiliza la informaci√≥n, es importante que esta informaci√≥n est√© resumida y sea f√°cilmente accesible para los usuarios de la aplicaci√≥n.

## ‚è≠Ô∏è Demo
Aqu√≠ puedes encontrar el enlace al video de la demo, donde se comenta la plataforma a nivel general y sin detalles t√©cnicos.
https://youtu.be/SYNQbkOoeDI

Pode√≠s encontrar tambien aqu√≠ la presentaci√≥n que emos usado en la demo:
[Presentaci√≥n](./files/JPG%20-%20PWP%20Presentacio%CC%81n.pdf)

## üë• Team Members
Nos conocimos en el trabajo, los tres trabaj√°bamos en la misma empresa y pensamos que √©ramos la combinaci√≥n perfecta para presentarnos a esta Hackathon. Nos llevamos de maravilla tanto en lo profesional como en lo personal.

Te presentamos a nuestro equipo:

**Joan Plaja**: Graduado en ingenier√≠a inform√°tica y DevOps Engineer certificado por AWS Solutions Architect. 

Puedes encontrar su perfil de LinkedIn en este enlace: https://www.linkedin.com/in/joan-josep-plaja/

**Pau Vilella**: Graduado en ingenier√≠a inform√°tica y DevOps Lead certificado por AWS Solutions Architect. 

Aqu√≠ tienes el enlace a su perfil de LinkedIn: https://www.linkedin.com/in/pau-vilella-stub-961b941b9/

**Gerard Lozano**: Estudiante de √∫ltimo curso de ingenier√≠a inform√°tica y Software Engineer. 

Puedes visitar su perfil de LinkedIn en este enlace: https://www.linkedin.com/in/gerardlozanotrias/

Esperamos que disfrutes de nuestro proyecto tanto como nosotros disfrutamos trabajando juntos en √©l.
## ‚è≠Ô∏è Trabajo futuro

Se nos ocurren muchas ideas para el futuro de este proyecto. Una de ellas es explorar maneras de reducir los costos al utilizar la API del ChatGPT. En nuestro proyecto, la grabaci√≥n en directo requiere muchas peticiones a la API de OpenAI, por lo que deber√≠amos investigar formas de reducir el n√∫mero de peticiones y, por ende, los costos asociados.

Uno de los temas que m√°s hemos investigado y perfeccionado es la diarizaci√≥n del audio, es decir, la separaci√≥n de las voces de los conversantes. Es un problema com√∫n que hemos encontrado en la comunidad, y sin el uso de GPU, se convierte en una tarea costosa. Por lo tanto, debemos seguir trabajando en ello para mejorar la precisi√≥n y reducir el costo computacional.

Aunque hemos trabajado r√°pidamente para desarrollar un producto s√≥lido en 15 d√≠as, todav√≠a hay margen de mejora en el c√≥digo tanto del backend como del frontend. Podemos seguir buenas pr√°cticas de programaci√≥n para optimizar el rendimiento y la eficiencia.

Adem√°s, podemos pensar en nuevas funcionalidades y mejorar las existentes. En cuanto a la arquitectura, debemos tomarnos el tiempo necesario para revisar la seguridad de toda la aplicaci√≥n y asegurarnos de cumplir todos los requisitos del well architecture framework. De esta manera, podemos garantizar un producto seguro y escalable.

## üéâ Conclusi√≥n

Estamos extremadamente orgullosos del trabajo que hemos realizado en esta hackathon y hemos aprendido much√≠simo durante estos d√≠as. Hemos trabajado en equipo como nunca antes y hemos sido capaces de tomar decisiones y ponerlas en pr√°ctica de manera eficiente.

Hemos aprendido varias lecciones importantes que nos gustar√≠a destacar. En primer lugar, la importancia de ser √°gil y capaz de pivotar, es decir, cambiar de rumbo cuando las cosas no van como se esperaba. Durante la hackathon, nos enfrentamos a muchos desaf√≠os y complicaciones que tuvimos que superar y en algunos casos, reconocimos que el camino que est√°bamos siguiendo no era el adecuado. Aprendimos que lo importante es no perder tiempo y pivotar r√°pidamente hacia una nueva direcci√≥n. El lema que adoptamos para la hackathon fue "El arte de pivotar". Tambi√©n aprendimos mucho sobre el manejo de archivos y, por supuesto, sobre inteligencia artificial.

En general, podemos decir que esta ha sido una experiencia muy enriquecedora que nos ha unido a√∫n m√°s como amigos y como equipo de trabajo. Hemos logrado consolidar un producto innovador que resuelve el problema planteado en muy pocos d√≠as. Estamos realmente satisfechos con lo que hemos logrado y esperamos seguir mejorando en el futuro.
